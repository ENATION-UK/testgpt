"""
æµ‹è¯•å¤šæ¨¡å‹é…ç½®åŠŸèƒ½
"""

import asyncio
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from autotest.services.multi_llm_service import MultiLLMService
from autotest.models import MultiModelConfig, ModelProviderConfig

async def test_multi_model_config():
    """æµ‹è¯•å¤šæ¨¡å‹é…ç½®åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤šæ¨¡å‹é…ç½®åŠŸèƒ½...")
    
    # åˆ›å»ºå¤šæ¨¡å‹æœåŠ¡å®ä¾‹
    service = MultiLLMService()
    
    # æµ‹è¯•1: è·å–é»˜è®¤é…ç½®
    print("\n1. æµ‹è¯•è·å–é»˜è®¤é…ç½®...")
    config = service._load_multi_model_config()
    print(f"é»˜è®¤é…ç½®: {json.dumps(config.model_dump(), indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•2: åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    print("\n2. æµ‹è¯•åˆ›å»ºè‡ªå®šä¹‰é…ç½®...")
    custom_config = MultiModelConfig(
        providers=[
            ModelProviderConfig(
                provider_id="openai_provider_1",
                provider_name="OpenAIæä¾›å•†1",
                model_type="openai",
                base_url="https://api.openai.com/v1",
                model="gpt-4o",
                temperature=0.7,
                max_tokens=None,
                api_keys=["sk-test-key-1", "sk-test-key-2"],
                rate_limit=2,
                is_active=True,
                current_key_index=0
            ),
            ModelProviderConfig(
                provider_id="openai_provider_2",
                provider_name="OpenAIæä¾›å•†2",
                model_type="openai",
                base_url="https://api.openai.com/v1",
                model="gpt-4o",
                temperature=0.7,
                max_tokens=None,
                api_keys=["sk-test-key-3", "sk-test-key-4"],
                rate_limit=2,
                is_active=True,
                current_key_index=0
            ),
            ModelProviderConfig(
                provider_id="deepseek_provider_1",
                provider_name="DeepSeekæä¾›å•†1",
                model_type="deepseek",
                base_url="https://api.deepseek.com/v1",
                model="deepseek-chat",
                temperature=0.7,
                max_tokens=None,
                api_keys=["sk-test-key-5", "sk-test-key-6"],
                rate_limit=2,
                is_active=True,
                current_key_index=0
            ),
            ModelProviderConfig(
                provider_id="deepseek_provider_2",
                provider_name="DeepSeekæä¾›å•†2",
                model_type="deepseek",
                base_url="https://api.deepseek.com/v1",
                model="deepseek-chat",
                temperature=0.7,
                max_tokens=None,
                api_keys=["sk-test-key-7", "sk-test-key-8"],
                rate_limit=2,
                is_active=True,
                current_key_index=0
            )
        ],
        current_provider_index=0
    )
    
    # ä¿å­˜é…ç½®
    service._save_multi_model_config(custom_config)
    print("è‡ªå®šä¹‰é…ç½®å·²ä¿å­˜")
    
    # æµ‹è¯•3: é‡æ–°åŠ è½½é…ç½®
    print("\n3. æµ‹è¯•é‡æ–°åŠ è½½é…ç½®...")
    loaded_config = service._load_multi_model_config()
    print(f"åŠ è½½çš„é…ç½®: {json.dumps(loaded_config.model_dump(), indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•4: æµ‹è¯•è½®è¯¢æœºåˆ¶
    print("\n4. æµ‹è¯•è½®è¯¢æœºåˆ¶...")
    for i in range(10):
        request_config = service._get_next_available_config(loaded_config)

        if request_config:
            print(f"ç¬¬{i+1}æ¬¡è¯·æ±‚: ä½¿ç”¨API key:{request_config.model_type}-- {request_config.api_key}")
        else:
            print(f"ç¬¬{i+1}æ¬¡è¯·æ±‚: æ²¡æœ‰å¯ç”¨çš„é…ç½®")
    
    # æµ‹è¯•5: è·å–é…ç½®çŠ¶æ€
    print("\n5. æµ‹è¯•è·å–é…ç½®çŠ¶æ€...")
    status = service.get_config_status()
    print(f"é…ç½®çŠ¶æ€: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•6: æµ‹è¯•APIæ¥å£
    print("\n6. æµ‹è¯•APIæ¥å£...")
    try:
        config_response = await service.get_multi_model_config()
        print(f"APIé…ç½®å“åº”: {json.dumps(config_response.model_dump(), indent=2, ensure_ascii=False)}")
    except Exception as e:
        print(f"APIæµ‹è¯•å¤±è´¥: {e}")
    
    print("\nâœ… å¤šæ¨¡å‹é…ç½®åŠŸèƒ½æµ‹è¯•å®Œæˆ!")

async def test_llm_integration():
    """æµ‹è¯•LLMé›†æˆ"""
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•LLMé›†æˆ...")
    
    service = MultiLLMService()
    
    # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
    from browser_use.llm.messages import SystemMessage, UserMessage, ContentPartTextParam
    
    messages = [
        SystemMessage(content=[ContentPartTextParam(text="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹")]),
        UserMessage(content="è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
    ]
    
    try:
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦çœŸå®çš„API keyæ‰èƒ½æµ‹è¯•
        print("æ³¨æ„ï¼šéœ€è¦çœŸå®çš„API keyæ‰èƒ½è¿›è¡Œå®Œæ•´æµ‹è¯•")
        print("è·³è¿‡å®é™…çš„LLMè°ƒç”¨æµ‹è¯•")
        
        # æµ‹è¯•é…ç½®è·å–
        config = service._load_multi_model_config()
        request_config = service._get_next_available_config(config)
        if request_config:
            print(f"æˆåŠŸè·å–è¯·æ±‚é…ç½®: {request_config.model_type}")
        else:
            print("æ— æ³•è·å–è¯·æ±‚é…ç½®")
            
    except Exception as e:
        print(f"LLMé›†æˆæµ‹è¯•å¤±è´¥: {e}")
    
    print("âœ… LLMé›†æˆæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_multi_model_config())
    asyncio.run(test_llm_integration()) 